{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First try of a* - DON'T LOOK THIS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "from collections import namedtuple\n",
    "from typing import List\n",
    "import heapq\n",
    "import time\n",
    "import heapq\n",
    "from typing import List, Tuple, Callable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Action type\n",
    "Action = namedtuple('Action', ['pos1', 'pos2'])\n",
    "\n",
    "# Define puzzle dimension\n",
    "PUZZLE_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance_2(state: np.ndarray, goal_state: np.ndarray) -> int:\n",
    "    \"\"\"Calculates the sum of Manhattan distances of tiles from their goal positions.\"\"\"\n",
    "    total_distance = 0\n",
    "    for val in range(1, PUZZLE_DIM**2):\n",
    "        current_pos = np.argwhere(state == val)[0]\n",
    "        goal_pos = np.argwhere(goal_state == val)[0]\n",
    "        total_distance += abs(current_pos[0] - goal_pos[0]) + abs(current_pos[1] - goal_pos[1])\n",
    "    return total_distance\n",
    "\n",
    "def available_actions(state: np.ndarray) -> List[Action]:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = []\n",
    "    if x > 0:\n",
    "        actions.append(Action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(Action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(Action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(Action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\"\"\"Apply the action to the state.\"\"\"\n",
    "def do_action(state: np.ndarray, action: Action) -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def a_star(start_state: np.ndarray, goal_state: np.ndarray, heuristic_func,) -> List[np.ndarray]:\n",
    "    \"\"\"Solve the puzzle using A*.\"\"\"\n",
    "    open_set = []\n",
    "    # Convert start state to tuple for immutability and comparison in sets\n",
    "    start_state_tuple = tuple(start_state.flatten())\n",
    "    goal_state_tuple = tuple(goal_state.flatten())\n",
    "\n",
    "    distance = heuristic_func(start_state, goal_state)\n",
    "    heappush(open_set, (distance, 0, start_state_tuple, []))\n",
    "\n",
    "    visited = set()\n",
    "    visited.add(start_state_tuple)\n",
    "\n",
    "    while open_set:\n",
    "        f, g, current_state_tuple, path = heappop(open_set)\n",
    "        current_state = np.array(current_state_tuple).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "\n",
    "        # Goal test\n",
    "        if np.array_equal(current_state, goal_state):\n",
    "            return path + [current_state]\n",
    "        \n",
    "\n",
    "        # Generate successors\n",
    "        for act in available_actions(current_state):\n",
    "            successor = do_action(current_state, act)\n",
    "            successor_tuple = tuple(successor.flatten())\n",
    "            if successor_tuple not in visited:\n",
    "                \n",
    "\n",
    "                new_path = path + [act]  # Copy and append current state current_state\n",
    "                #new_cost = g + 1\n",
    "                new_cost = g + 0.00 * len(new_path)#(new_priority, len(new_path), new_path, successor), heuristic_func(successor, goal_state)\n",
    "\n",
    "                #heappush(open_set, (new_cost + heuristic_func(successor, goal_state), new_cost, successor_tuple, new_path))\n",
    "                heapq.heappush(open_set, (new_cost, len(new_path), successor_tuple, new_path))\n",
    "\n",
    "                visited.add(successor_tuple)\n",
    "\n",
    "    return None  # No solution found\n",
    "\n",
    "\n",
    "def a_star_better_priority(initial_state: np.ndarray, goal_state: np.ndarray, heuristic_func, length_criteria=0) -> list:\n",
    "    \"\"\"Solves the n^2-1 puzzle using A* search with the given heuristic function.\"\"\"\n",
    "    open_set = []\n",
    "    distance=heuristic_func(initial_state, goal_state)\n",
    "    heapq.heappush(open_set, (distance, 0, [], initial_state))\n",
    "\n",
    "    visited = set()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    best_state = None\n",
    "\n",
    "    while open_set:\n",
    "        _, move_count, path, current_state = heapq.heappop(open_set) #f, g, current_state_tuple, path\n",
    "\n",
    "        if np.array_equal(current_state, goal_state):\n",
    "            return path, current_state\n",
    "\n",
    "        state_tuple = tuple(current_state.flatten())\n",
    "        if state_tuple in visited:\n",
    "            continue\n",
    "        visited.add(state_tuple)\n",
    "\n",
    "        for act in available_actions(current_state):\n",
    "            successor = do_action(current_state, act)\n",
    "            new_path = path + [act]\n",
    "            new_priority = heuristic_func(successor, goal_state) + length_criteria * len(new_path)\n",
    "            heapq.heappush(open_set, (new_priority, len(new_path), new_path, successor))\n",
    "\n",
    "            current_distance = heuristic_func(successor, goal_state)\n",
    "            if current_distance < min_distance:\n",
    "                min_distance = current_distance\n",
    "                best_state = successor\n",
    "\n",
    "        if time.time() - start_time >= 5:\n",
    "            print(f\"Current depth: {move_count}, queue size: {len(open_set)}, current min distance: {min_distance}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    print(f\"Best state with min distance: {min_distance}\")\n",
    "    print(best_state)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Randomizing: 100%|██████████| 1000/1000 [00:00<00:00, 55624.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "[[ 3  8  5 15  2 18]\n",
      " [28  7 34  4 14 10]\n",
      " [ 1  6 20 16 31 22]\n",
      " [23 11 33  0 21 12]\n",
      " [13 19  9 32 30 17]\n",
      " [26 25 24 27 29 35]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Goal_State = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "\n",
    "RANDOMIZE_STEPS = 1000 #100_000  # 100 steps\n",
    "state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    state = do_action(state, choice(available_actions(state)))\n",
    "\n",
    "print(\"Initial State:\")\n",
    "print(state)\n",
    "\n",
    "# solution_path = a_star(state, Goal_State, manhattan_distance_2)\n",
    "# print(\"\\nSolution Steps:\")\n",
    "# print(f\"Solution found in {len(solution_path)} moves.\")\n",
    "# for step in solution_path:\n",
    "#     print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current depth: 165, queue size: 2201, current min distance: 35\n",
      "Current depth: 172, queue size: 4474, current min distance: 35\n",
      "Current depth: 178, queue size: 6830, current min distance: 35\n",
      "Current depth: 180, queue size: 9056, current min distance: 35\n",
      "Current depth: 184, queue size: 11288, current min distance: 35\n",
      "Current depth: 186, queue size: 13381, current min distance: 35\n",
      "Current depth: 190, queue size: 15366, current min distance: 35\n",
      "Current depth: 194, queue size: 17409, current min distance: 35\n",
      "Current depth: 201, queue size: 19416, current min distance: 35\n",
      "Current depth: 207, queue size: 21370, current min distance: 35\n",
      "Current depth: 158, queue size: 23484, current min distance: 33\n",
      "Current depth: 195, queue size: 25915, current min distance: 31\n",
      "Current depth: 212, queue size: 27978, current min distance: 31\n",
      "Current depth: 232, queue size: 30103, current min distance: 31\n",
      "Current depth: 216, queue size: 32372, current min distance: 25\n",
      "Current depth: 218, queue size: 34580, current min distance: 25\n",
      "Current depth: 223, queue size: 36729, current min distance: 25\n",
      "Current depth: 229, queue size: 38617, current min distance: 25\n",
      "Current depth: 222, queue size: 40409, current min distance: 21\n",
      "Current depth: 234, queue size: 42519, current min distance: 21\n",
      "Current depth: 326, queue size: 44718, current min distance: 13\n",
      "Current depth: 370, queue size: 46817, current min distance: 6\n",
      "Current depth: 372, queue size: 48810, current min distance: 6\n",
      "Current depth: 370, queue size: 50932, current min distance: 6\n",
      "Current depth: 419, queue size: 53143, current min distance: 6\n",
      "Current depth: 412, queue size: 55330, current min distance: 6\n",
      "Current depth: 441, queue size: 57630, current min distance: 6\n",
      "Current depth: 380, queue size: 59917, current min distance: 6\n",
      "Current depth: 388, queue size: 61917, current min distance: 6\n",
      "Current depth: 397, queue size: 64000, current min distance: 6\n",
      "Current depth: 398, queue size: 66203, current min distance: 6\n",
      "Current depth: 403, queue size: 68258, current min distance: 6\n",
      "Current depth: 402, queue size: 70187, current min distance: 6\n",
      "Current depth: 406, queue size: 72375, current min distance: 6\n",
      "Current depth: 410, queue size: 74694, current min distance: 6\n",
      "Current depth: 412, queue size: 77070, current min distance: 6\n",
      "Current depth: 414, queue size: 79364, current min distance: 6\n",
      "Current depth: 426, queue size: 81389, current min distance: 6\n",
      "Current depth: 420, queue size: 83680, current min distance: 6\n",
      "Current depth: 422, queue size: 85985, current min distance: 6\n",
      "Current depth: 431, queue size: 88248, current min distance: 6\n",
      "Current depth: 430, queue size: 90506, current min distance: 6\n",
      "Current depth: 432, queue size: 92692, current min distance: 6\n",
      "Current depth: 443, queue size: 94895, current min distance: 6\n",
      "Current depth: 444, queue size: 97164, current min distance: 6\n",
      "Current depth: 448, queue size: 99346, current min distance: 6\n",
      "Current depth: 454, queue size: 101402, current min distance: 6\n",
      "Current depth: 464, queue size: 103659, current min distance: 6\n",
      "Current depth: 476, queue size: 105858, current min distance: 6\n",
      "Current depth: 359, queue size: 108109, current min distance: 6\n",
      "Current depth: 367, queue size: 110517, current min distance: 6\n",
      "Current depth: 373, queue size: 112771, current min distance: 6\n",
      "Current depth: 400, queue size: 115065, current min distance: 6\n",
      "Current depth: 371, queue size: 117429, current min distance: 6\n",
      "Current depth: 385, queue size: 119734, current min distance: 6\n",
      "Current depth: 375, queue size: 122004, current min distance: 6\n",
      "Current depth: 377, queue size: 124303, current min distance: 6\n",
      "Current depth: 377, queue size: 126544, current min distance: 6\n",
      "Current depth: 379, queue size: 128814, current min distance: 6\n",
      "Current depth: 381, queue size: 131165, current min distance: 6\n",
      "Current depth: 381, queue size: 133479, current min distance: 6\n",
      "Current depth: 383, queue size: 135772, current min distance: 6\n"
     ]
    }
   ],
   "source": [
    "solution, final_state = a_star_better_priority(state, Goal_State, manhattan_distance_2,0)\n",
    "if solution:\n",
    "    print(f\"Solution found in {len(solution)} moves.\")\n",
    "    print(final_state)\n",
    "    # print(solution)\n",
    "else:\n",
    "    print(\"No solution found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(start: np.ndarray, goal: np.ndarray) -> List[np.ndarray]:\n",
    "    \"\"\"Implementazione dell'algoritmo di ricerca Greedy usando la distanza di Manhattan.\"\"\"\n",
    "    # Coda di priorità per esplorare gli stati più promettenti (minore distanza)\n",
    "    frontier = []\n",
    "    heapq.heappush(frontier, (manhattan_distance_2(start, goal), start))  # (f, stato)\n",
    "\n",
    "    visited = set()  # Insieme degli stati esplorati\n",
    "    start_tuple = tuple(start.flatten())\n",
    "    visited.add(start_tuple)  # Aggiungi lo stato iniziale esplorato come tupla\n",
    "\n",
    "    parent_map = {start_tuple: None}  # Per ricostruire il percorso\n",
    "\n",
    "    while frontier:\n",
    "        # Prendi il nodo con la minima distanza di Manhattan\n",
    "        _, current_state_tuple = heapq.heappop(frontier)\n",
    "        current_state = np.array(current_state_tuple).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "\n",
    "        # Se abbiamo raggiunto il goal, ricostruiamo il percorso\n",
    "        if np.array_equal(current_state, goal):\n",
    "            path = []\n",
    "            while current_state is not None:\n",
    "                path.append(current_state)\n",
    "                current_state = parent_map[tuple(current_state.flatten())]\n",
    "            return path[::-1]  # Ritorna il percorso dal start al goal\n",
    "\n",
    "        # Esplora i successori\n",
    "        for action in available_actions(current_state):\n",
    "            new_state = do_action(current_state, action)\n",
    "            new_state_tuple = tuple(new_state.flatten())\n",
    "            if new_state_tuple not in visited:\n",
    "                visited.add(new_state_tuple)  # Marca come esplorato\n",
    "                heapq.heappush(frontier, (manhattan_distance_2(new_state, goal), new_state_tuple))\n",
    "                parent_map[new_state_tuple] = current_state  # Salva il predecessore\n",
    "\n",
    "    return None  # Nessuna soluzione trovata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution_path=greedy_search(state, Goal_State)\n",
    "\n",
    "# if solution_path:\n",
    "#     print(f\"Solution found in {len(solution_path)} moves.\")\n",
    "# else:\n",
    "#     print(\"No solution found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
