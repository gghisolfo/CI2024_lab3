{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Copyright **`(c)`** 2024 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free under certain conditions — see the [`license`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "from collections import namedtuple\n",
    "from typing import List\n",
    "import heapq\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Action type\n",
    "Action = namedtuple('Action', ['pos1', 'pos2'])\n",
    "\n",
    "# Define puzzle dimension\n",
    "PUZZLE_DIM = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> List[Action]:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = []\n",
    "    if x > 0:\n",
    "        actions.append(Action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(Action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(Action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(Action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\"\"\"Apply the action to the state.\"\"\"\n",
    "def do_action(state: np.ndarray, action: Action) -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(state: np.ndarray, goal: np.ndarray) -> int:\n",
    "    \"\"\"Calcola la distanza di Manhattan tra lo stato attuale e quello finale.\"\"\"\n",
    "    distance = 0\n",
    "    for num in range(1, PUZZLE_DIM**2):  # Ignoriamo 0 (lo spazio vuoto)\n",
    "        x1, y1 = np.where(state == num)\n",
    "        x2, y2 = np.where(goal == num)\n",
    "        distance += abs(x1[0] - x2[0]) + abs(y1[0] - y2[0])\n",
    "    return distance\n",
    "\n",
    "def manhattan_distance_2(state: np.ndarray, goal_state: np.ndarray) -> int:\n",
    "    \"\"\"Calculates the sum of Manhattan distances of tiles from their goal positions.\"\"\"\n",
    "    total_distance = 0\n",
    "    for val in range(1, PUZZLE_DIM**2):\n",
    "        current_pos = np.argwhere(state == val)[0]\n",
    "        goal_pos = np.argwhere(goal_state == val)[0]\n",
    "        total_distance += abs(current_pos[0] - goal_pos[0]) + abs(current_pos[1] - goal_pos[1])\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star(start_state: np.ndarray, goal_state: np.ndarray, heuristic_func,) -> List[np.ndarray]:\n",
    "    \"\"\"Solve the puzzle using A*.\"\"\"\n",
    "    open_set = []\n",
    "    # Convert start state to tuple for immutability and comparison in sets\n",
    "    start_state_tuple = tuple(start_state.flatten())\n",
    "    goal_state_tuple = tuple(goal_state.flatten())\n",
    "\n",
    "    distance = heuristic_func(start_state, goal_state)\n",
    "    heappush(open_set, (distance, 0, start_state_tuple, []))\n",
    "\n",
    "    visited = set()\n",
    "    visited.add(start_state_tuple)\n",
    "\n",
    "    while open_set:\n",
    "        f, g, current_state_tuple, path = heappop(open_set)\n",
    "        current_state = np.array(current_state_tuple).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "\n",
    "        # Goal test\n",
    "        if np.array_equal(current_state, goal_state):\n",
    "            return path + [current_state]\n",
    "        \n",
    "\n",
    "        # Generate successors\n",
    "        for act in available_actions(current_state):\n",
    "            successor = do_action(current_state, act)\n",
    "            successor_tuple = tuple(successor.flatten())\n",
    "            if successor_tuple not in visited:\n",
    "                new_path = path + [act]  # Copy and append current state current_state\n",
    "                #new_cost = g + 1\n",
    "                new_cost = heuristic_func(successor, goal_state) + 0 * len(new_path)#(new_priority, len(new_path), new_path, successor)\n",
    "\n",
    "                #heappush(open_set, (new_cost + heuristic_func(successor, goal_state), new_cost, successor_tuple, new_path))\n",
    "                heapq.heappush(open_set, (new_cost, len(new_path), successor_tuple, new_path))\n",
    "\n",
    "                visited.add(successor_tuple)\n",
    "\n",
    "    return None  # No solution found\n",
    "\n",
    "\n",
    "def a_star_better_priority(initial_state: np.ndarray, goal_state: np.ndarray, heuristic_func, length_criteria=0) -> list:\n",
    "    \"\"\"Solves the n^2-1 puzzle using A* search with the given heuristic function.\"\"\"\n",
    "    open_set = []\n",
    "    distance=heuristic_func(initial_state, goal_state)\n",
    "    heapq.heappush(open_set, (distance, 0, [], initial_state))\n",
    "\n",
    "    visited = set()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    min_distance = float('inf')\n",
    "    best_state = None\n",
    "\n",
    "    while open_set:\n",
    "        _, move_count, path, current_state = heapq.heappop(open_set) #f, g, current_state_tuple, path\n",
    "\n",
    "        if np.array_equal(current_state, goal_state):\n",
    "            return path, current_state\n",
    "\n",
    "        state_tuple = tuple(current_state.flatten())\n",
    "        if state_tuple in visited:\n",
    "            continue\n",
    "        visited.add(state_tuple)\n",
    "\n",
    "        for act in available_actions(current_state):\n",
    "            successor = do_action(current_state, act)\n",
    "            new_path = path + [act]\n",
    "            new_priority = heuristic_func(successor, goal_state) + length_criteria * len(new_path)\n",
    "            heapq.heappush(open_set, (new_priority, len(new_path), new_path, successor))\n",
    "\n",
    "            current_distance = heuristic_func(successor, goal_state)\n",
    "            if current_distance < min_distance:\n",
    "                min_distance = current_distance\n",
    "                best_state = successor\n",
    "\n",
    "        if time.time() - start_time >= 5:\n",
    "            print(f\"Current depth: {move_count}, queue size: {len(open_set)}, current min distance: {min_distance}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    print(f\"Best state with min distance: {min_distance}\")\n",
    "    print(best_state)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(start: np.ndarray, goal: np.ndarray) -> List[np.ndarray]:\n",
    "    \"\"\"Implementazione dell'algoritmo di ricerca Greedy usando la distanza di Manhattan.\"\"\"\n",
    "    # Coda di priorità per esplorare gli stati più promettenti (minore distanza)\n",
    "    frontier = []\n",
    "    heapq.heappush(frontier, (manhattan_distance(start, goal), start))  # (f, stato)\n",
    "\n",
    "    visited = set()  # Insieme degli stati esplorati\n",
    "    start_tuple = tuple(start.flatten())\n",
    "    visited.add(start_tuple)  # Aggiungi lo stato iniziale esplorato come tupla\n",
    "\n",
    "    parent_map = {start_tuple: None}  # Per ricostruire il percorso\n",
    "\n",
    "    while frontier:\n",
    "        # Prendi il nodo con la minima distanza di Manhattan\n",
    "        _, current_state_tuple = heapq.heappop(frontier)\n",
    "        current_state = np.array(current_state_tuple).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "\n",
    "        # Se abbiamo raggiunto il goal, ricostruiamo il percorso\n",
    "        if np.array_equal(current_state, goal):\n",
    "            path = []\n",
    "            while current_state is not None:\n",
    "                path.append(current_state)\n",
    "                current_state = parent_map[tuple(current_state.flatten())]\n",
    "            return path[::-1]  # Ritorna il percorso dal start al goal\n",
    "\n",
    "        # Esplora i successori\n",
    "        for action in available_actions(current_state):\n",
    "            new_state = do_action(current_state, action)\n",
    "            new_state_tuple = tuple(new_state.flatten())\n",
    "            if new_state_tuple not in visited:\n",
    "                visited.add(new_state_tuple)  # Marca come esplorato\n",
    "                heapq.heappush(frontier, (manhattan_distance(new_state, goal), new_state_tuple))\n",
    "                parent_map[new_state_tuple] = current_state  # Salva il predecessore\n",
    "\n",
    "    return None  # Nessuna soluzione trovata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Randomizing: 100%|██████████| 1000/1000 [00:00<00:00, 59805.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "[[11  4 38 10 21 34 27]\n",
      " [ 8  2 12 28  6  7 19]\n",
      " [23  3 17 15 13 14 35]\n",
      " [ 1  9  5 44 39 33 40]\n",
      " [43 32 18 45 24 48 26]\n",
      " [16 25 42 37 46 41 47]\n",
      " [22 29  0 31 36 30 20]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial State:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[1;32m---> 13\u001b[0m solution_path \u001b[38;5;241m=\u001b[39m \u001b[43ma_star\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGoal_State\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanhattan_distance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSolution Steps:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolution found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(solution_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m moves.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[75], line 30\u001b[0m, in \u001b[0;36ma_star\u001b[1;34m(start_state, goal_state, heuristic_func)\u001b[0m\n\u001b[0;32m     28\u001b[0m new_path \u001b[38;5;241m=\u001b[39m path \u001b[38;5;241m+\u001b[39m [act]  \u001b[38;5;66;03m# Copy and append current state current_state\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#new_cost = g + 1\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m new_cost \u001b[38;5;241m=\u001b[39m \u001b[43mheuristic_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuccessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoal_state\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(new_path)\u001b[38;5;66;03m#(new_priority, len(new_path), new_path, successor)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#heappush(open_set, (new_cost + heuristic_func(successor, goal_state), new_cost, successor_tuple, new_path))\u001b[39;00m\n\u001b[0;32m     33\u001b[0m heapq\u001b[38;5;241m.\u001b[39mheappush(open_set, (new_cost, \u001b[38;5;28mlen\u001b[39m(new_path), successor_tuple, new_path))\n",
      "Cell \u001b[1;32mIn[74], line 7\u001b[0m, in \u001b[0;36mmanhattan_distance\u001b[1;34m(state, goal)\u001b[0m\n\u001b[0;32m      5\u001b[0m     x1, y1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(state \u001b[38;5;241m==\u001b[39m num)\n\u001b[0;32m      6\u001b[0m     x2, y2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(goal \u001b[38;5;241m==\u001b[39m num)\n\u001b[1;32m----> 7\u001b[0m     distance \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(x1[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m x2[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distance\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Goal_State = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "\n",
    "RANDOMIZE_STEPS = 1000 #100_000  # 100 steps\n",
    "state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    state = do_action(state, choice(available_actions(state)))\n",
    "\n",
    "print(\"Initial State:\")\n",
    "print(state)\n",
    "\n",
    "solution_path = a_star(state, Goal_State, manhattan_distance)\n",
    "print(\"\\nSolution Steps:\")\n",
    "print(f\"Solution found in {len(solution_path)} moves.\")\n",
    "# for step in solution_path:\n",
    "#     print(step)\n",
    "# solution_path=greedy_search(state, Goal_State)\n",
    "# solution, final_state = a_star_better_priority(state, Goal_State, manhattan_distance,0)\n",
    "# if solution:\n",
    "#     print(f\"Solution found in {len(solution)} moves.\")\n",
    "#     print(final_state)\n",
    "#     print(solution)\n",
    "# else:\n",
    "#     print(\"No solution found.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
